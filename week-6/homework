1.Area to use.√
2. When to use.×
3. How to use.×
4. Compare each.√


1.Batch Norm
对一个个batch的所有图片的一个channel进行规范化
应用场景:
mini-batch 比较大的情况下使用, BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便

缺点:
数据分布影响效果,mini-batch 需要比较大才能合理训练数据的均值和方差,对小batch效果不好
很难应用在训练长度不同的RNN模型上



2.Layer Norm
对一张图片的所有channel进行规范化
应用场景:
可以用于 小mini-batch场景、动态网络场景和 RNN(循环神经网络)
特点:不受mini-batch数据分布影响,LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。




3.Instance Norm
应用场景:
用于图像风格迁移.在生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，因此可以先把图像在 channel 层面归一化，然后再用目标风格图片对应 channel 的均值和标准差“去归一化”，以期获得目标图片的风格



4.Group Norm
应用场景:
适用于占用显存比较大的任务，例如图像分割


比较:
LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；
BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。
图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立.
Group Norm是对Instance Norm和Layer Norm的折中.
